{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. The Blocker Fraud Company\n",
    "\n",
    "**Blocker Fraud Company** is a specialized company in fraud detection on financial transactions through mobile devices. It has the \"Blocker Fraud\" service, wich guarantee the block of fraudulent transactions.\n",
    "\n",
    "The company business model is service type, with monetization made by performance of the provided service. The user pay a fixed fee on the fraud detection success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Expansion Strategy in Brazil\n",
    "\n",
    "Aiming to expand business in Brazil it has adopted the following strategy:\n",
    "\n",
    "* The company will receive 25% of the value of each transaction detected as *fraud*.\n",
    "* The company will receive 5% of the value of each transaction detected as *fraud*, but the transaction is *legitimate*.\n",
    "* The company will return 100% of the value to the customer, for each transaction detected as legitimate, however a transaction is a fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Context\n",
    "There is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets.\n",
    "\n",
    "We present a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Content\n",
    "PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\n",
    "\n",
    "This synthetic dataset is scaled down 1/4 of the original dataset and it is created just for Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Goal\n",
    "1. What is the model's *precision* and *accuracy*?\n",
    "2. How reliable is the model in classifying transactions as *legitimate* or *fraudulent*?\n",
    "3. What is the expected billing by the company if we classify 100% of data transactions with the model?\n",
    "4. What is the loss expected by company in case of model failure ?\n",
    "5. What is the profit expected by the **Blocker Fraud Company** when using model?\n",
    "\n",
    "`Disclaimer: The following context is completely fictional, the company, the context, the CEO and the business questions.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Data\n",
    "\n",
    "Data provided by Kaggle: [Synthetic Financial Datasets for Fraud Detection](https://www.kaggle.com/ntnu-testimon/paysim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Analysis\n",
    "\n",
    "This solution will use descriptive statistics and data visualization to find key figures in understanding the distribution, count, and relationship between variables. Since the goal of the project to make predictions on the fraud's detection, classification algorithms from the supervised learning family of machine learning models will be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Evaluation\n",
    "\n",
    "The project will conclude with the evaluation of the machine learning model selected with a validation data set. The output of the predictions can be checked through a confusion matrix, and metrics such as accuracy, precision, recall, F1 and Kappa scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ae7f409b8de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# importing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv('PS_20174392719_1491204439457_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows and columns\n",
    "print(df.shape)\n",
    "\n",
    "# check first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headers\n",
    "\n",
    "| Feature        | Description                                                  |\n",
    "| :------------- | :----------------------------------------------------------- |\n",
    "| step           | maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation). |\n",
    "| type           | Transaction type (CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER) |\n",
    "| amount         | amount of the transaction in local currency                  |\n",
    "| nameOrig       | customer who started the transaction                         |\n",
    "| oldbalanceOrg  | initial balance before the transaction                       |\n",
    "| newbalanceOrig | new balance after the transaction                            |\n",
    "| nameDest       | customer who is the recipient of the transaction             |\n",
    "| oldbalanceDest | initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants) |\n",
    "| newbalanceDest | new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants) |\n",
    "| isFraud        | This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system |\n",
    "| isFlaggedFraud | The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200K in a single transaction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bdff2da2fb18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# describe data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# describe data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check if there any missing values that might need to be imputed or removed. Luckily the data is pretty clean so no need to worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify fraudulent actions a dataset with fraud detection will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Step\n",
    "1. Fraud shoul occour for transactions made before day 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Transactions amount\n",
    "1. Fraud occur with high transactions amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Transactions Origin\n",
    "1. Fraud not occour when `oldbalanceOrg` is equal to zero.\n",
    "2. Fraud not occour when `newbalanceOrig` is equal to zero.\n",
    "3. Fraud occour when the balance difference berofe and after is different from the transaction amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Transactions Destiny\n",
    "1. Fraud not occour when `oldbalanceDest` is equal to zero.\n",
    "2. Fraud not occour when `newbalanceOrig` is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Transactions Type\n",
    "1. Fraud occour with cash out.\n",
    "2. Fraud occour with transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Relationship\n",
    "\n",
    "Understand relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudulents transactions represents 0.13% of the DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "total2 = float(len(df))\n",
    "sns.set(style=\"darkgrid\")\n",
    "print((df.isFraud.value_counts()/df.isFraud.count()) * 100)\n",
    "ax = sns.countplot(data=df, x='isFraud', palette='Set3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud occours only in 2 type of transactions: **TRANSFER** and **CASH_OUT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting most common fraud type\n",
    "total = float(len(fraud))\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.set(style=\"darkgrid\")\n",
    "print(fraud.type.value_counts())\n",
    "ax = sns.countplot(data=fraud, x='type', palette='Set1')\n",
    "plt.title('Fraud by transfer and cash out', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting no fraud transactions\n",
    "total = float(len(fraud))\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.set(style=\"darkgrid\")\n",
    "print(no_fraud.type.value_counts())\n",
    "ax = sns.countplot(data=no_fraud, x='type', palette='Set1')\n",
    "plt.title('No Fraud', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudulent transactions represent a 1% loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_transaction = df.groupby('isFraud')['amount'].sum()\n",
    "print((amount_transaction/amount_transaction.sum())*100)\n",
    "amount_transaction.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong correlation between the columns: `oldbalanceOrg`, `newbalanceOrig`, `oldbalanceDest` and `newbalanceDest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "\n",
    "Show the possible values that we can expect to see in a variable, along with how likely they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The step is right-skewed with less median than mean. \n",
    "* The `fraud` data has no obvious clustering, it is a *Uniform Distribution* and *symmetric skew*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and median\n",
    "df_step_mean = df.step.mean()\n",
    "df_step_median = df.step.median()\n",
    "fraud_step_mean = fraud.step.mean()\n",
    "fraud_step_median = fraud.step.median()\n",
    "\n",
    "# ploting first histogram\n",
    "fig, axs = plt.subplots(nrows= 2, ncols=1, figsize=(20,6), sharex=True)\n",
    "fig.suptitle('Steps in hour', fontsize=20)\n",
    "sns.distplot(df.step, kde=False, ax=axs[0], color='green')\n",
    "axs[0].set_xlabel('Step in Overral Transactions')\n",
    "axs[0].axvline(df_step_median, color = 'r', linewidth = 3, label = 'median')\n",
    "axs[0].axvline(df_step_mean, color='b', linewidth = 3, label = 'mean')\n",
    "axs[0].legend()\n",
    "\n",
    "# ploting second histogram\n",
    "sns.distplot(fraud.step, kde=False, ax=axs[1])\n",
    "axs[1].set_xlabel('Step in Fraud transactions')\n",
    "axs[1].axvline(fraud_step_median, color = 'g', linewidth = 1, label = 'median')\n",
    "axs[1].axvline(fraud_step_mean, color='black', linewidth = 1, label = 'mean')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends\n",
    "\n",
    "Define pattern of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows= 3, ncols=1, figsize=(20,6), sharex=True, sharey=True)\n",
    "fig.suptitle('Transactions', fontsize=20)\n",
    "# amount linechart\n",
    "sns.lineplot(data = df, x='step', y='amount', ax=axs[0], color='green', ci=None)\n",
    "axs[0].set_ylabel('Amount')\n",
    "\n",
    "# origin balance linechart\n",
    "sns.lineplot(data = df, x='step', y='oldbalanceOrg', ax=axs[1], color='blue', ci=None, label='Old Balance')\n",
    "sns.lineplot(data = df, x='step', y='newbalanceOrig', ax=axs[1], color='black', ci=None, label='New Balance')\n",
    "axs[1].set_ylabel('Balance Origin')\n",
    "axs[1].legend()\n",
    "\n",
    "# destination balance linechart\n",
    "sns.lineplot(data = df, x='step', y='newbalanceOrig', ax=axs[2], color='purple', ci=None, label='Old Balance')\n",
    "sns.lineplot(data = df, x='step', y='newbalanceDest', ax=axs[2], color='tomato', ci=None, label='New Balance')\n",
    "axs[2].set_ylabel('Balance Destination')\n",
    "axs[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables Filtering\n",
    "\n",
    "This step aims to remove the outliers from the dataset. As previously checked in the descriptive statistics, some features have a huge range of values, particularly the amount, destination balance before the transaction `oldbalanceDest` and the destination balance after the transaction `newbalanceDest`, as shown in the boxplot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers\n",
    "Creating a function to drop outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_outliers('amount', df)\n",
    "df = drop_outliers('oldbalanceDest', df)\n",
    "df = drop_outliers('newbalanceDest', df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(var: str, dataset: pd.DataFrame):\n",
    "\n",
    "    # find Q1, Q3 e IQR\n",
    "    Q1 = np.quantile(dataset[var], .25)\n",
    "    Q3 = np.quantile(dataset[var], .75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # calculates the outliers boundaries through statistical relationship\n",
    "    low = Q1 - 1.5 * IQR\n",
    "    high = Q3 + 1.5 * IQR\n",
    "\n",
    "    dados_resultado = dataset.loc[(dataset[var] > low) & (dataset[var] < high)]\n",
    "    return dados_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Outliers with violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot 1\n",
    "sns.boxplot(df.amount)\n",
    "plt.xlabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot 2\n",
    "sns.boxplot(df.oldbalanceOrg)\n",
    "plt.xlabel('Old Balance Origin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows= 5, ncols=1, figsize=(20,6), sharex=True, sharey=True)\n",
    "fig.suptitle('Transactions x step', fontsize=20)\n",
    "# amount \n",
    "sns.scatterplot(data = df2, x='step', y='amount', hue='isFraud', ax=axs[0], color='green')\n",
    "\n",
    "\n",
    "# origin balance \n",
    "sns.scatterplot(data = df2, x='step', y='oldbalanceOrg', hue='isFraud', ax=axs[1], color='blue', label='Old Balance')\n",
    "sns.scatterplot(data = df2, x='step', y='newbalanceOrig', ax=axs[2], hue='isFraud',  color='black', label='New Balance')\n",
    "\n",
    "\n",
    "# destination balance \n",
    "sns.scatterplot(data = df2, x='step', y='newbalanceOrig', ax=axs[3], hue='isFraud', color='purple', label='Old Balance')\n",
    "sns.scatterplot(data = df2, x='step', y='newbalanceDest', ax=axs[4], hue='isFraud', color='tomato', label='New Balance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen earlier, Fraud Transactions only occours in Transactions and Cash Out type. A new Data will be created based on these two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[(df.type == 'TRANSFER') | (df.type == 'CASH_OUT')]\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables\n",
    "In this step, dummy variables are created to deal with the categorical variables. Dummy variables will turn the categories per variable into its own binary identifier.\n",
    "* 0 = Cash out\n",
    "* 1 = Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.type = pd.get_dummies(df2.type)\n",
    "print(len(df2))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('dataset/df2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "For start the machine learning model, the data needs to be split into train and validation sets.  In this split 20% of the data is reserved for the final validations, while 80% is kept for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading df2 data\n",
    "df2 = pd.read_csv('dataset/df2.csv')\n",
    "\n",
    "# Y1 is the targeting column and X1 has the rest\n",
    "X = df2.drop(columns=['nameOrig', 'nameDest', 'isFraud', 'isFlaggedFraud'], axis=1)\n",
    "Y = df2['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into chunks\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Now it's time to create some models. For this project three common algorithms will be used to make predictions. \n",
    "\n",
    "The respective modeules for Logistic Regression, Decision Trees and Random Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "The first model is logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrpred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_val, lrpred)\n",
    "precision = precision_score(Y_val, lrpred)\n",
    "recall = recall_score(Y_val, lrpred)\n",
    "f1 = f1_score(Y_val, lrpred)\n",
    "\n",
    "logistic_regression = pd.DataFrame(\n",
    "    [['Logistic Regression', acc, precision, recall, f1]],\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtpred = dt.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_val, dtpred)\n",
    "precision = precision_score(Y_val, dtpred)\n",
    "recall = recall_score(Y_val, dtpred)\n",
    "f1 = f1_score(Y_val, dtpred)\n",
    "\n",
    "decision_tree = pd.DataFrame(\n",
    "    [['Decision Tree Classifier', acc, precision, recall, f1]],\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 42).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict = rf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_val, rf_predict)\n",
    "precision = precision_score(Y_val, rf_predict)\n",
    "recall = recall_score(Y_val, rf_predict)\n",
    "f1 = f1_score(Y_val, rf_predict)\n",
    "\n",
    "random_forest = pd.DataFrame(\n",
    "    [['Random Forest', acc, precision, recall, f1]],\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predict = xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_val, xgb_predict)\n",
    "precision = precision_score(Y_val, xgb_predict)\n",
    "recall = recall_score(Y_val, xgb_predict)\n",
    "f1 = f1_score(Y_val, xgb_predict)\n",
    "\n",
    "xg_boost = pd.DataFrame(\n",
    "    [['XGB', acc, precision, recall, f1]],\n",
    "    columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "Confusion matrix of the results with the true values on the y axis and predicted values along the x axis. Since the diagonals are lighter in color and have higher numbers, the accuracy is going to be high since those are the True Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "cmLR = confusion_matrix(Y_val, lrpred)\n",
    "cm_LR = pd.DataFrame(cmLR, columns=['predicted_non_fraud', 'predicted_fraud'], index=['actual_non_fraud', 'actual_fraud'])\n",
    "\n",
    "# decision trees\n",
    "cmDT = confusion_matrix(Y_val, dtpred)\n",
    "cm_DT = pd.DataFrame(cmDT, columns=['predicted_non_fraud', 'predicted_fraud'], index=['actual_non_fraud', 'actual_fraud'])\n",
    "\n",
    "# random forest\n",
    "cmRF = confusion_matrix(Y_val, rf_predict)\n",
    "cm_RF = pd.DataFrame(cmRF, columns=['predicted_non_fraud', 'predicted_fraud'], index=['actual_non_fraud', 'actual_fraud'])\n",
    "\n",
    "# XGBoost\n",
    "cmXG = confusion_matrix(Y_val, xgb_predict)\n",
    "cm_XG = pd.DataFrame(cmXG, columns=['predicted_non_fraud', 'predicted_fraud'], index=['actual_non_fraud', 'actual_fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes\",fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "sns.heatmap(cm_LR,annot=True,cmap=\"Greens\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(\"Decision Tree Confusion Matrix\")\n",
    "sns.heatmap(cm_DT,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "sns.heatmap(cm_RF,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "sns.heatmap(cm_XG,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_scores = pd.concat([logistic_regression, decision_tree, random_forest, xg_boost], ignore_index=True)\n",
    "ml_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Answering questions based on the best performing Machine Learning Model: **Decision Tree** and **Random Forest**. \n",
    "\n",
    "* The company will receive 25% of the value of each transaction detected as *fraud*.\n",
    "* The company will receive 5% of the value of each transaction detected as *fraud*, but the transaction is *legitimate*.\n",
    "* The company will return 100% of the value to the customer, for each transaction detected as legitimate, however a transaction is a fraud.\n",
    "\n",
    "Median amount of a transaction: \\$7,4871.8 (we're using the median because the amount distribution is highly skewed).\n",
    "\n",
    "Portfolio: 6,362,620 transactions (fraudulent + non fraudulent).\n",
    "* True positive amount = [0.25 x (median amount) x (number of TP transactions)]\n",
    "* False positive amount = [0.05 x (median amount) x (number of FP transactions)]\n",
    "* False negative amount = [1 x (median amount) x (number of FN transactions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join predictions on the test set in order to calculate the business performance\n",
    "Y_val['predictions'] = dtpred\n",
    "Y_val['amount']  = X_val['amount']\n",
    "\n",
    "# the company receive 25% of each transaction value truly detected as fraud\n",
    "fraud_detected = Y_val[(Y_val['isFraud'] == 1) & (Y_val['predictions'] == 1)]\n",
    "fraud_detected_amount = fraud_detected[['amount', 'isFraud', 'predictions']].groupby(['isFraud', 'predictions']).sum().reset_index()\n",
    "fraud_detected_amount['to_receive'] = fraud_detected_amount['amount'] * 0.25\n",
    "\n",
    "# the company receive 5% of each transaction value truly detected as fraud\n",
    "fraud_detected_leg = Y_val[(Y_val['isFraud'] == 0) & (Y_val['predictions'] == 1)]\n",
    "fraud_detected_leg_amount = fraud_detected_leg[['amount', 'isFraud', 'predictions']].\\\n",
    "    groupby(['isFraud', 'predictions']).sum().reset_index()\n",
    "fraud_detected_leg_amount['to_receive'] = fraud_detected_leg_amount['amount']*0.05\n",
    "\n",
    "# the company gives back 100% of the value for the customer in each transaction detected as Legitimate\n",
    "fraud_not_detected = Y_val[(Y_val['isFraud'] == 1) & (Y_val['predictions'] == 0)]\n",
    "fraud_not_detected_amount = fraud_not_detected[['amount', 'isFraud', 'predictions']].\\\n",
    "    groupby(['isFraud', 'predictions']).sum().reset_index()\n",
    "\n",
    "# print results\n",
    "print('The company will receive ${:,.2f} due to transactions truly detected as fraud'.format(fraud_detected_amount['to_receive'][0]))\n",
    "print('The company will receive ${:,.2f} due to transactions detected as fraud, but actually legitimate'.\\\n",
    "      format(fraud_detected_leg_amount['to_receive'][0]))\n",
    "print('The company will give back ${:,.2f} due to transactions detected as legitimate, but actually fraud'.\\\n",
    "      format(fraud_not_detected_amount['amount'][0]))\n",
    "# the company profit\n",
    "profit = fraud_detected_amount['to_receive'][0] + fraud_detected_leg_amount['to_receive'][0] - fraud_not_detected_amount['amount'][0]\n",
    "print(f'The Blocker Fraud Company forecasted profit: ${profit:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
